from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
import os
import json
from dotenv import load_dotenv
import asyncio

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LLM Testing Tool API",
    description="LLM API ì—°ë™ì„ ìœ„í•œ ë°±ì—”ë“œ ì„œë²„",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("CORS_ORIGINS", "http://localhost:5173").split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic ëª¨ë¸ ì •ì˜
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    model: Optional[str] = "gpt-4"
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2048

class ChatResponse(BaseModel):
    content: str
    model: str
    usage: Optional[dict] = None

# ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "message": "LLM Testing Tool API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": "2024-07-10T00:00:00Z"
    }

# ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/chat-stream")
async def chat_stream(request: ChatRequest):
    """
    ì±„íŒ… ë©”ì‹œì§€ë¥¼ ë°›ì•„ì„œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    í˜„ì¬ëŠ” ì—ì½” ì‘ë‹µìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = None
        for message in reversed(request.messages):
            if message.role == "user":
                user_message = message.content
                break
        
        if not user_message:
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ì ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì—ì½” ì‘ë‹µ ìƒì„± (ì‹¤ì œ LLM API ì—°ë™ ì „ í…ŒìŠ¤íŠ¸ìš©)
        echo_response = f"ì—ì½” ì‘ë‹µ: {user_message}"
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        async def generate_response():
            # ì‘ë‹µì„ ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë°
            chunks = [echo_response[i:i+20] for i in range(0, len(echo_response), 20)]
            
            for i, chunk in enumerate(chunks):
                # SSE í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì „ì†¡
                data = {
                    "content": chunk,
                    "is_complete": i == len(chunks) - 1,
                    "model": request.model
                }
                
                yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                
                # ì‹¤ì œ LLM API í˜¸ì¶œ ì‹œì—ëŠ” ì—¬ê¸°ì„œ ì§€ì—°ì´ ë°œìƒ
                await asyncio.sleep(0.1)
        
        return StreamingResponse(
            generate_response(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")

# ì„¤ì • ì •ë³´ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/config")
async def get_config():
    """í˜„ì¬ ì„œë²„ ì„¤ì • ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        "backend_port": os.getenv("BACKEND_PORT", "8000"),
        "host": os.getenv("HOST", "0.0.0.0"),
        "cors_origins": os.getenv("CORS_ORIGINS", "http://localhost:5173"),
        "debug": os.getenv("DEBUG", "false").lower() == "true",
        "environment": os.getenv("ENVIRONMENT", "development")
    }

# ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("BACKEND_PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    debug = os.getenv("DEBUG", "false").lower() == "true"
    
    print(f"ğŸš€ ì„œë²„ ì‹œì‘: http://{host}:{port}")
    print(f"ğŸ“– API ë¬¸ì„œ: http://{host}:{port}/docs")
    print(f"ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ: {debug}")
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=debug,
        log_level="info"
    ) 